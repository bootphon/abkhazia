# -*- coding: utf-8 -*-
"""
Created on Thu Nov 10 14:45:45 2016

@author: xcao

Data preparation for the KCSS corpus
This script has designed to process data in Korean. The data are freely available to the research community (contact whyun@kmu.ac.kr or
kyoon@ynu.ac.kr)
This database consists  of  40  hours  of  sponteaneous speech  recorded  from 40  participants. Inspired by the Buckeye corpus.

"""

import os
import re
import codecs
import shutil


#input_dir_KCSS_wavs = "/home/xcao/cao/multilingual/KCSS/sounds"
input_dir_KCSS_wavs = "/home/xcao/cao/multilingual/KCSS/label/test"
input_dir_KCSS_trs = "/home/xcao/cao/multilingual/KCSS/label/test"
output_dir = "/home/xcao/cao/multilingual/abkhazia_KCSS/"

#list all wav files
def list_wav(input_dir):
    file_list = []
    for dirpath, dirs, files in os.walk(input_dir):
        for f in files:
            m_file = re.match("^(.*)\.wav$", f)
            if m_file:
                file_list.append(os.path.join(dirpath, f))
    return file_list
    
#list all transcription files
def list_trn(input_dir):
    file_list = []
    for dirpath, dirs, files in os.walk(input_dir):
        for f in files:
            m_file = re.match("(.*)TextGrid$", f)
            if m_file:
                file_list.append(os.path.join(dirpath, f))
    return file_list

#This function creates a "wavs" folder if it doesn' exist and a sym link for each wav file
def make_wavs(input_dir, output_dir):
    directory = os.path.join(output_dir, "wavs")
    if not os.path.exists(directory):
        os.makedirs(directory)
    else:
        shutil.rmtree(directory)
        os.makedirs(directory)
    wav_list = list_wav(input_dir)
    for wav_file in wav_list:
        bname_wav = os.path.basename(wav_file)
        dst_wav = os.path.join(directory, bname_wav)
        #shutil.copy2(wav_file, dst_wav)
        os.symlink(wav_file, dst_wav)

#here, we are using the pronunciation transcription
#Use item[7] if orthographic utterance transcription (instead of item[4])
#For item[5], must replace by end of file
def make_segments(input_dir, output_dir):
    trn_list = list_trn(input_dir)
    outfile = open(os.path.join(output_dir, "segments.txt"), 'w')
    for trn_file in trn_list:
        data_utt = []
        gather = False
        index = 0
        basename = os.path.splitext(os.path.basename(trn_file))[0]
        infile = codecs.open(trn_file, 'r', 'utf-16-be')
        #extract from TextGrid file the tier that corresponds to the utterance transcription
        for line in infile:
            line = line.rstrip()
            match_begin_interval = re.match("(.*)item \[4\](.*)", line)
            if match_begin_interval:
                gather = True
            else:
                match_end_interval = re.match(".*item \[5\].*", line)
                if match_end_interval:
                    gather = False
                elif line and gather:
                    data_utt.append(line)
        for i in data_utt:
            match_onset = re.match("            xmin = (.*)", i)
            if match_onset:
                onset = match_onset.group(1)
            else:
                match_offset = re.match("            xmax = (.*)", i)
                if match_offset:
                    offset = match_offset.group(1)
                    new_index = index + 1
                    utt_ID = basename + "-sent" + str(new_index)
                    wav_file = basename + ".wav"
                    outfile.write(utt_ID + ' ' + wav_file  + ' ' + onset + ' ' + offset + '\n')
                    index = new_index
        infile.close()
    outfile.close()

        
def make_speaker(output_dir):
    infile = open(os.path.join(output_dir, "segments.txt"), 'r')
    outfile = open(os.path.join(output_dir, "utt2spk.txt"), 'w')
    for line in infile:
        line = line.rstrip()
        match_line = re.match("(.*)\s(.*)\s(.*)\s(.*)", line)
        if match_line:
            utt_ID = match_line.group(1)
            match_speaker_ID = re.match("([a-z0-9][a-z0-9][a-z0-9])(.*)", utt_ID)
            if match_speaker_ID:
                speaker_id = match_speaker_ID.group(1)
                outfile.write(utt_ID + ' ' + speaker_id + '\n')
                

#This part nees to be optimized because takes too long to run (many hours)
#here, we are using the pronunciation transcription
#Use item[6] if word orthographic transcription (instead of item[3]
#Use item[7] if utterance orthographic transcription (instead of item[4]
def make_transcription (input_dir, output_dir):
    infile_segments = open(os.path.join(output_dir, "segments.txt"), 'r')
    outfile = codecs.open(os.path.join(output_dir, "text.txt"), 'w', 'utf-16-be')
    trn_list = list_trn(input_dir)
    for line in infile_segments:
        line = line.rstrip()
        match_line = re.match("(.*)\s(.*)\s(.*)\s(.*)", line)
        if match_line:
            utt_ID = match_line.group(1)
            trn_file = match_line.group(2)
            trn_file_without_ext = trn_file.replace (".wav", "")
            onset_utt = match_line.group(3)
            offset_utt = match_line.group(4)
            outfile.write(utt_ID + ' ')
            for trn in trn_list:
                base_name_trn = os.path.splitext(os.path.basename(trn))[0]
                if (base_name_trn == trn_file_without_ext):
                    data_utt = []
                    gather = False
                    infile = codecs.open(trn, 'r', 'utf-16-be')
                    #extract from TextGrid file the tier that corresponds to the romanized word transcription
                    for line in infile:
                        line = line.rstrip()
                        match_begin_interval = re.match("(.*)item \[3\](.*)", line)
                        if match_begin_interval:
                            gather = True
                        else:
                            match_end_interval = re.match(".*item \[4\].*", line)
                            if match_end_interval:
                                gather = False
                            elif line and gather:
                                data_utt.append(line)
                    word_list = []
                    gather = False
                    #start from line 6 to by pass the xmin = 0 of the whole file, and not the xmin = 0 of the word
                    for i in data_utt[5:]:
                        regex_onset = r"            xmin = " + re.escape(onset_utt)
                        match_onset = re.match(regex_onset, i)
                        if match_onset:
                            gather = True
                        else:
                            regex_offset = r"            xmin = " + re.escape(offset_utt)
                            match_offset = re.match(regex_offset, i)
                            if match_offset:
                                gather = False
                            elif i and gather:
                                word_list.append(i)
                    for w in word_list:
                        match_word = re.match("(.*)text = \"(.*)\"", w)
                        if match_word:
                            word = match_word.group(2)
                            word = word.replace ('-', '')
                            outfile.write(word + ' ')
                    outfile.write ('\n')
                    infile.close()
    infile_segments.close()
    outfile.close()

def make_lexicon(output_dir):
    infile_text = codecs.open(os.path.join(output_dir, "text.txt"), 'r', 'utf-16-be')
    outfile = codecs.open(os.path.join(output_dir, "lexicon.txt"), 'w', 'utf-16-be')
    list_lines = infile_text.read().splitlines()
    dict_word = {}
    for line in list_lines:
        if re.match('^$', line):
            index_empty_line = list_lines.index(line)
            print(index_empty_line)
        else:
            line = line.strip()
            line_match = re.match("(s.*[0-9]+) (.*)", line)
            if line_match:
                utt = line_match.group(2)
                list_word = utt.split(' ')
                for i in list_word:
                    if i == "":
                        continue
                    else:
                        if i in dict_word:
                            dict_word[i] = dict_word[i] + 1
                        else:
                            dict_word[i] = 1
    for w in sorted(dict_word, key=dict_word.get, reverse=True):
        outfile.write (w + '\t')
        match_romanized = re.match("[a-zA-Z0-9]+", w)
        if match_romanized:
            phones = []
            phones = re.findall('..?', w)
            #print('starting phone from lexicon')
            for p in phones:
                p = p.replace ('E', 'e')
                p = p.replace ('W', 'w')
                p = p.replace ('Y', 'y')
                outfile.write (p + ' ')
            outfile.write ('\n')
        else:
            #replace non-speech labels by SPN or NSN
            match_nsn = re.match("(<NOISE.*|<LAUGH>)", w)
            if match_nsn:
                phone_nsn = re.sub('<NOISE.*|<LAUGH>', 'NSN', w)
                outfile.write (phone_nsn + '\n')
            else:
                match_spn = re.match("(<IVER>|<VOCNOISE.*|<LAUGH.+|<UNKNOWN.*|<PRIVATE.*)", w)
                if match_spn:
                    phone_spn = re.sub('(<IVER>|<VOCNOISE.*|<LAUGH.+|<UNKNOWN.*|<PRIVATE.*)', 'SPN', w)
                    outfile.write (phone_spn + '\n')
                else:
                    outfile.write (w + '\n')
    infile_text.close()
    outfile.close()
    

def make_phones(output_dir):
    outfile = open(os.path.join(output_dir, "phones.txt"), 'w')
    outfile_silences = open(os.path.join(output_dir, "silences.txt"), 'w')
    outfile_variants = open(os.path.join(output_dir, "variants.txt"), 'w')
    outfile.write('p0 p\n')
    outfile.write('ph pʰ\n')
    outfile.write('pp p*\n')
    outfile.write('t0 t\n')
    outfile.write('th tʰ\n')
    outfile.write('tt t*\n')
    outfile.write('k0 k\n')
    outfile.write('kh kʰ\n')
    outfile.write('kk k*\n')
    outfile.write('ll l\n')
    outfile.write('s0 s\n')
    outfile.write('ss s*\n')
    outfile.write('hh h\n')
    outfile.write('c0 ʨ\n')
    outfile.write('ch ʨʰ\n')
    outfile.write('cc ʨ*\n')
    outfile.write('mm m\n')
    outfile.write('nn n\n')
    outfile.write('ng ŋ\n')
    outfile.write('ii i\n')
    outfile.write('ye je\n')
    outfile.write('ee e\n')
    outfile.write('ya ja\n')
    outfile.write('aa a\n')
    outfile.write('yv jə\n')
    outfile.write('xx ɨ\n')
    outfile.write('yu ju\n')
    outfile.write('vv ə\n')
    outfile.write('yo jo\n')
    outfile.write('uu u\n')
    outfile.write('wi wi\n')
    outfile.write('oo o\n')
    outfile.write('we we\n')
    outfile.write('wa wa\n')
    outfile.write('wv wə\n')
    outfile.write('xi ɨi')
    outfile_silences.write ('NSN NSN\n')
    outfile.close()
    outfile_silences.close()
    outfile_variants.close()
    
    
#make_wavs(input_dir_KCSS_wavs, output_dir)
#make_segments(input_dir_KCSS_trs, output_dir)
#make_speaker(output_dir)
#make_transcription(input_dir_KCSS_trs, output_dir)
#make_lexicon (output_dir)
#make_phones(output_dir)


